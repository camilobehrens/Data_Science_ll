{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f0a965f",
   "metadata": {},
   "source": [
    "# 13. Evaluación de Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43969b82",
   "metadata": {},
   "source": [
    "## 13.1 Matriz de Confisión\n",
    "\n",
    "La **matriz de confusión** es una herramienta que se utiliza para evaluar el desempeño de un modelo de clasificación en problemas de Aprendizaje Supervisado. A diferencia de métricas como el Accuracy, que pueden ser engañosas si el conjunto de datos está desbalanceado, la metriz de confisión proporciona una visión más detallada de los errores cometidos por el modelo.\n",
    "\n",
    "\n",
    "### Estructura de la Matriz de Confusión\n",
    "\n",
    "En una matriz de confusión, los valores reales se organizan en filas y los valores predichos en columnas. La matriz tiene la siguiente estructura básica en un problema de clasificación binaria:\n",
    "\n",
    "|          | **Predicción Positiva** | **Predicción Negativa** |\n",
    "|--------------|--------------|--------------|\n",
    "| **Clase Positiva (1)** | Verdaderos Positivos (TP) | Falsos Negativos (FN) |\n",
    "| **Clase Negativa (0)** | Falsos Positivos (FP) | Verdaderos Negativos (TN) |\n",
    "\n",
    "\n",
    "* **TP (True Positive):** la clase predicha es positiva y corresponde a la clase real positiva.\n",
    "\n",
    "* **TN (True Negative):** la clase predicha es negativa y corresponde a la clase real negativa.\n",
    "\n",
    "* **FP (False Postive):** la clase predicha es positiva pero en realidad es negativa (Error Tipo I).\n",
    "\n",
    "* **FN (False Negative):** la clase predicha es negativa pero en realidad es positiva (Error Tipo II).\n",
    "\n",
    "\n",
    "### Cálculo de la Matriz de Confusión\n",
    "\n",
    "Para calcular una matriz de confusión se requiere un conjunto de datos de prueba. Se predicen las etiquetas de todas las observaciones y se comparan con los valores reales. Cada predicción se clasifica en una de las cuatro categorías mencionadas (TP, TN, FP, FN) y estos valores se colocan en la matriz.\n",
    "\n",
    "\n",
    "### Métricas Derivadas de la Matriz de Confusión.\n",
    "\n",
    "A Partir de la matriz de confusión, se pueden derivar varias métricas para evaluar el desempeño de un modelo de clasificación:\n",
    "\n",
    "* **Accuracy:** mide el porcentaje de predicciones correctas sobre el total de predicciones. \n",
    "\n",
    "        Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "* **Precision:** indica cuántas de las predicciones positivas realizadas son realmente positivas. \n",
    "\n",
    "        Precision = TP/(TP + FP)  \n",
    "\n",
    "* **Sensibilidad (Recall):** aka tasa de verdaderos positivos, mide la capacidad del modelo para identificar correctamente las instancias positivas.\n",
    "\n",
    "        \n",
    "        Recall = TP/(TP + FN)\n",
    "\n",
    "* **Especificidad:** aka tasa de Verdaderos Negetivos, evalúa la capacidad del modelo para identificar correctamente las instancias negativas. \n",
    "\n",
    "        Specifiicity = TN/(TN + FP)\n",
    "\n",
    "* **F1 Score:** es la media armónica entre la precisión y la sensibilidad. Es útil cuando se busca un balance entre ambas métricas.\n",
    "\n",
    "        F1= 2(Precision x Recall)/(Precision + Recall)\n",
    "\n",
    "\n",
    "\n",
    "### Utilidad en la Evaluación de Modelos\n",
    "\n",
    "La matriz de confusión es particularmente útil para identificar que tipo de errores está cometiendo un modelo y cómo mejorar su rendimiento. Permite detectar problemas como falsos positivos y falsos negativos, lo que crucial en aplicaciones donde ciertos tipos de errores tienen más impacto que otros (por ejemplo, en la detección de fraudes o enfermedades).\n",
    "\n",
    "\n",
    "## Descripción Paso a Paso: Cómo Calcaular la Matriz de Confusión con un Dataset de Prueba\n",
    "\n",
    "### Paso 1: Preparar el Dataset de Prueba\n",
    "\n",
    "Para calcular una matriz de confusión se necesita un dataset de prueba o validación que contenga los valores reales de la variable objetivo (es decir, la categoría verdadera de cada observación). Este dataset debe estar separado del dataset de entrenamiento para evitar resultados sesgados y obtener una evaluación objetiva del modelo.\n",
    "\n",
    "\n",
    "### Paso 2: Realiza Predicciones\n",
    "\n",
    "Una vez que el modelo de clasificación ha sido entrenado, el siguiente paso es realizar predicciones sobre cada instancia del dataset de prueba. Esto implica aplicar el modelo a las características de cada fila del dataset para generar una predicción (ya sea positiva o negativa en el caso de un problema binario).\n",
    "\n",
    "\n",
    "### Paso 3: Comparar las Predicciones con los Valores Reales\n",
    "\n",
    "Para cada instancia en el dataset de prueba, se compara el valor predicho con el valor real. Esta comparación es clave para clasificar las predicciones en las siguientes categorias:\n",
    "\n",
    "* **Verdaderos Positivos (TP):** predicciones que son positivas y corresponde a valores reales positivos.\n",
    "\n",
    "* **Falsos Positivos (FP):** predicciones que son positivas pero en realidad son falsas (Error Tipo I).\n",
    "\n",
    "* **Verdaderos Negativos (TN):** predicciones que son negativas y corresponden con los valores reales negativos.\n",
    "\n",
    "* **Falsos Negativos (FN):** predicciones que son negativas pero el valor real es positivo (Error Tipo II).\n",
    "\n",
    "\n",
    "### 4: Crear la Matriz de Confusión\n",
    "\n",
    "Una vez que se han clasificado todas las predicciones, estos valores se organizan en una tabla o metriz de la siguiente manera:\n",
    "\n",
    "|          | **Predicción Positiva** | **Predicción Negativa** |\n",
    "|--------------|--------------|--------------|\n",
    "| **Clase Positiva (1)** | Verdaderos Positivos (TP) | Falsos Negativos (FN) |\n",
    "| **Clase Negativa (0)** | Falsos Positivos (FP) | Verdaderos Negativos (TN) |\n",
    "\n",
    "Cada celda de la matriz representa el número de instancias que caen en cada categoría (TP, TN, FP, FN).\n",
    "\n",
    "\n",
    "\n",
    "### Paso 5: Interpretar los Resultados\n",
    "\n",
    "La matriz de confisión ofrece una representación clara del desempeño del modelo. Aquí es donde se puede observar cuántas instancias fueron clasificadas correctamente (TP y TN) y cuántas fueron clasificadas incorrectamente (FP y FN). Dependiendo de la aplicación, diferentes tipos de errores pueden tener más o menos relevancia.\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "* Un alto número de **Falsos Positivos** puede ser problemático en un modelo de detección de fraude si muchas transacciones legítimas se etiquetan erróneamente como fraudulentas.\n",
    "\n",
    "* Un alto número de **Falsos Negativos (FN)** podría ser grave en un modelo de diagnóstico médico, donde una enfermedad no es detectada cuando realmente está presente.\n",
    "\n",
    "\n",
    "\n",
    "### Conslusión\n",
    "\n",
    "El cálculo de la matriz de confusión es un paso esencial para evaluar el desempeño de modelos de clasificación. No solo permite visualizar qué tan bien el modelo está realizando las predicciones, sino que también proporciona una base para el cálculo de métricas adicionales que ofrecen una mejor comprensión de sus fortalezas y debilidades.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2657e72",
   "metadata": {},
   "source": [
    "## 13.2 Análisis y Métricas\n",
    "\n",
    "### Métricas Derivadas\n",
    "\n",
    "**Explicación de Métricas Clave**\n",
    "\n",
    "\n",
    "### Precision\n",
    "\n",
    "La *precision* es una métrica utilizada para evalluar la calidad de las predicciones positivas realizadas por un modelo de clasificación. Indica qué proporción de las predicciones que el modelo ha clasificado como positivas son realmente verdaderas. En terminos simples mide cuántos de los verdaderos positivos sobre el total de predicciones positivas (verdaderos y falsos positivos) han sido identificados correctamente.\n",
    "\n",
    "La fórmula de la *precision* es la siguiente:\n",
    "\n",
    "            Precision = TP / (TP + FP)\n",
    "\n",
    "donde:\n",
    "* **TP (True Positive):** verdaderos positivos, es decir, predicciones correctas de la clase positiva.\n",
    "\n",
    "* **FP (False Positive):** falsos positivos, predicciones incorrectas donde el modelo clasificó erróneamente una instancia negativa como positiva.\n",
    "\n",
    "Una *alta precision* significa que el modelo comote pocos falsos positivos. Es especialmente importante en aplicaciones donde los *Falsos Positivos* tienen un alto costo, como en la detección de fraudes o en diagnóstico médico.\n",
    "\n",
    "\n",
    "### Recall\n",
    "\n",
    "También llamada *Sensibilidad* o *Tasa de Verdaderos Positivos*, el recall mide la capacidad del modelo para identificar correctamente todas las instancias positivas. En otras palabraas, indica qué proporción de las verdaderas instancias positivas fueron correctamente clasificacdas como positivas por el modelo.\n",
    "\n",
    "La fórmula del recall es:\n",
    "\n",
    "        Recall = TP / (TP + FN)\n",
    "\n",
    "donde:\n",
    "* TP: Verdaderoos positivos\n",
    "* FN: falsos negativos, es decir, instancias que el modelo clasificó incorrectamente como negativas cuando en realidad son positivas.\n",
    "\n",
    "Una *alta sensibilidad* es crucial en situaciones donde es importante detectar todas las instancias positivas, como en pruebas de diagnóstico de enfermedades graves. En estos casos, los *falsos negativos* pueden tener consecuencias graves, por lo que se prioriza identificar la mayor cantidad posible de verdaderos positivos.\n",
    "\n",
    "\n",
    "\n",
    "### Tasa de Error\n",
    "\n",
    "La *tasa de error* o *Error rate* mide la proporción de predicciones incorrectas sobre el total de predicciones. Es una métrica global que indica el rendimiento general del modelo, reflejando tanto *falsos positivos* como *falsos negativos*.\n",
    "\n",
    "La fórmula de la tasa de error es:\n",
    "\n",
    "        Tasa de Error = FP + FN / (TP + TN + FP + FN)\n",
    "\n",
    "La tasa de error proporciona una visión general de cuántas predicciones totales fueron incorrectas. Cuanto *menor sea la tasa de error*, mejor será el desempeño del modelo. Sin embargo, no siempre es suficiente evaluar un modelo solo por esta métrica, ya que en detasets desbalanceados (con muchas más instancial de una clase que de otra), la tasa de error puede ser baja pero las métricas como la precision o el recall pueden revelar deficiencias importantes.\n",
    "\n",
    "\n",
    "### Comparación y Contexto\n",
    "\n",
    "* **Precision** se enfoca en la calidad de las predicciones positivas, minimizando los falsos positivos.\n",
    "\n",
    "* **Sensibilidad** se concentra en identificar la mayor cantidad posible de verdaderos positivos, minimizando los falsos negativos.\n",
    "\n",
    "* **Tasa de Error es una métrica más general que mide el rendimiento global del modelo, considerando tanto los falsos positivos como los falsos negativos.\n",
    "\n",
    "\n",
    "### Conclusión\n",
    "\n",
    "Estas métricas clave son esenciales para evaluar adecuadamente el desempeño de los modelos de clasificación. Según el problema a resolver, algunas métricas serán más relevantes que otras. Por ejemplo, en un situación donde los falsos positivos sean críticos (como en detección de fraudes), la precisión será la métrica más importante, mientras que en casos donde es crucial no perder ningún positivo (como en detección de enfermedades), la sensibilidad será la métrica prioritaria."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "soundscape311",
   "language": "python",
   "name": "soundscape311"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
