{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a05af5dc",
   "metadata": {},
   "source": [
    "# 12. Reducción y Agrupación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e588280",
   "metadata": {},
   "source": [
    "## 12.1 Introducción a la Reducción\n",
    "\n",
    "La reducción de dimensionalidad es una técnica clave en Data Science que permite simplificar los conjuntos de datos con muchas variables, sin perder las caracteríssticas más importante. Este proceso mejora la eficiencia de los modelos, facilita la visualización de datos y ayuda a prevenir el sobreajuste. El sobreajuste ocurre cuando un modelo es demasiado complejo y se adapta excesivamente a los datos de entrenamiento, lo que disminuye su capacidad de generalización a nuevos datos. Reduciendo la dimensionalidad, se consigue que el modelo sea más simple, interpretable y rápido de entrenar. Al disminuir el número de variables, también se reduce la cantidad de ruido presente en los datos, lo que mejora el rendimiento general del modelo.\n",
    "\n",
    "Existen dos enfoques principales para la reducción de dimensionalidad:\n",
    "\n",
    "**1. Selección de características (Feature Selection):** se selecciona un subconjunto de las variables más importantes, eliminando las irrelevantes o redundantes.\n",
    "\n",
    "**2. Extracción de características (Feature Extraction):** se generan nuevas variables (combinaciones lineales de las originales) que retienen la mayor cantidad de información posible, pero en un espacio de menor dimensión.\n",
    "\n",
    "Entre las ténicas más populares de reducción de dimensionalidad se encuentran el Análisis de Componentes Principales (PCA), Kernel PCA y t-SNE.\n",
    "\n",
    "\n",
    "### Beneficios clave\n",
    "\n",
    "La reducción de dimensionalidad aporta varios beneficios significativos, especialmente en proyectos de machine learning y análisis de datos:\n",
    "\n",
    "**1. Menor tiempo de entrenamiento:** al trabajar con menos variables, los algoritmos de ML procesan la información más rápidamente, reduciendo el tiempo necesario para entrenar los modelos.\n",
    "\n",
    "**2. Menor uso de recursos:** un número menor de dimensiones implica un uso más eficiente de recursos computacionales, tanto en memoria como en procesamiento.\n",
    "\n",
    "**3. Prevención del sobreajuste:** al eliminar características irrelevantes, se reduce la complejidad del modelo, disminuyendo así el riesgo de que este se ajuste demasiado a los datos de entrenamiento.\n",
    "\n",
    "**4. Mejora en la visualización:** al reducir el número de dimensiones es más fácil crear visualizaciones comprensibles, permitiendo una interpretación más clara de los datos.\n",
    "\n",
    "**5. Manejo de multicalineales:** la reducción de dimensionalidad permite abordar problemas como la multicolinealidad, donde las variables están altamente correlacionadas entre sí, afectando negativamente al rendimiento del modelo. \n",
    "\n",
    "**6. Eliminación de ruido:** al enfocarse en las variables más importantes, se reduce el ruido presente en los datos, mejorando la calidad del análisis y la predicción.\n",
    "\n",
    "\n",
    "En resumen, la reducción de la dimensionalidad es una técnica poderosa que no solo mejora la eficiencia y el rendimiento de los modelos, sino que también facilita el análisis y la interpretación de los datos, siendo especialmente útil en la visualización y en la prevención del sobreajuste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc41eab",
   "metadata": {},
   "source": [
    "## 12.2 Análisis PCA\n",
    "\n",
    "El Análisis de Componentes Principales (PCA) es una técnica de reducción de dimensionalidad ampliamente utilizada en el análisis de datos y machine learning. Su objetivo principal es transformar un conjunto de variables posiblemente correlacionadas en un conjunto más pequeño de variables no correlacionadas, conocidas como componentes principales. Estos componentes retienen la mayor cantidad posible de la varianza presente en los datos originales.\n",
    "\n",
    "### ¿Cómo funciona el PCA?\n",
    "\n",
    "El PCA transforma los datos preyectándolos en un espacio de menor dimensión, con los siguientes pasos:\n",
    "\n",
    "1. **Estandarización de los datos:** es importante que las variables estén en la misma escala para que ninguna domine el análisis.\n",
    "\n",
    "2. **Cálculo de la matriz de covarianza:** esta matriz muestra cómo varían las variables entre sí.\n",
    "\n",
    "3. **Obtención de los autovalores y autovectores:** los autovectores son las direcciones (componentes principales) en las que se maximiza la varianza, y los autovalores representan la cantidad de varianza que corresponde a cada componente.\n",
    "\n",
    "4. **Proyección en el nuevo espacio:** los datos originales se proyectan en un nuevi conjunto de ejes, correspondientes a los componentes principales.\n",
    "\n",
    "\n",
    "### Beneficios de PCA\n",
    "\n",
    "1. **Reducción de dimensionalidad:** al convertir un conjunto de variables correlacionadas en un conjunto más peuqeño de componentes principales, se reduce la cantidad de datos a procesar, lo que mejora la eficiencia del análisis sin perder información relevante.\n",
    "\n",
    "2. **Eliminación de redundancia:** como PCA transforma las variabes correlacionadas en componentes no correlacionados, se elimina la redundancia en los datos.\n",
    "\n",
    "3. **Facilita la visualización:** PCA permite visualizar datos complejos en dos o  tres dimensiones, lo cual es útil para detectar patrones relacionados entre variables.\n",
    "\n",
    "4. **Prevención del sobreajuste:** al reducir la dimensionalidad, se simplifica el modelo, lo que ayuda a evitar el sobre ajuste a los datos de entrenamiento.\n",
    "\n",
    "\n",
    "### Aplicaciones del PCA\n",
    "\n",
    "El PCA tiene diversas aplicaciones en diferentes campos:\n",
    "\n",
    "* **Análisis de datos exploratorios:** PCA permite identificar patrones y tendencias ocultas en los datos.\n",
    "\n",
    "* **Comprensión de datos:** es útil reducir el tamaño de los conjuntos de datos sin perder información significativa.\n",
    "\n",
    "* **Visualización:** en datasets con muchas dimensiones, PCA facilita la representación gráfica de los datos, haciendo posible visualizar relaciones y agrupamientos\n",
    "\n",
    "* **Preprocesamiento para ML:** PCA se utiliza para transformar los datos y eliminar variables irrelevantes o ruidosas antes de aplicar modelos de ML.\n",
    "\n",
    "\n",
    "En resumen, el PCA es una técnica poderosa para reducir dimensionalidad y mejorar la eficiencia en el análisis de datos, especialmente cuando se trabaja con un gran número de variables correlacionadas.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5a9af9",
   "metadata": {},
   "source": [
    "## 12.3 Algoritmo K-means\n",
    "\n",
    "### ¿Qué es k-means?\n",
    "\n",
    "K-means es un algoritmo de aprendizaje no supervisado utilizado para resolver problemas de agrupación de datos o clustering. El objetivo de este algoritmo es dividir un conjunto de datos en k grupos o clústeres, donde k es un parámentro definido por el usuario. Los datos dentro de cada grupo comparten características similares, mientras que los distintos grupos tienen características deferenciadas. K-means es una herramienta poderosa para identificar patrones en los datos y es especialmente útil en escenarios donde no hay etiquetas predefinidas para clasificar los datos.\n",
    "\n",
    "K-means es ampliamente usado en diversas aplicaciones, como la segmentación de clientes, el análisis de patrones de imágenes, y la clasificación de documentos.\n",
    "\n",
    "\n",
    "### Beneficios de k-means:\n",
    "\n",
    "* **Simplicidad y eficiencia:** k-means es fácilde implementar y computaciionalmente eficiente, lo que lo hace ideal para grandes conjuntos de datos.\n",
    "\n",
    "* **Flexibilidad:** puede ser aplicado a una variedad de problemas donde se necesite agrupar datos no etiquetados en categorías o grupos.\n",
    "\n",
    "\n",
    "### Funcionamiento de k-means\n",
    "\n",
    "El algoritmo k-means sigue un proceso iterativo para agrupar los datos. A continuación se describen los pasos del proceso:\n",
    "\n",
    "1. **Selección del número de clústeres (k):** el usuario define cuántos grupos se desean crear. Este valor puede determinarse con métodos como el 'método del codo', que analiza la variación total dentro de los grupos.\n",
    "\n",
    "2. **Inicialización de los centroides:** se seleccionan de manera aleatoria k puntos en el espacio de datos, que actúan como centros iniciales de los clústeres. Estos puntos se denominan centroides.\n",
    "\n",
    "3. **Asignación de datos a los centroides:** cada punto de datos se asigna al clúster cuyo centroide está más cercano, medido por la distancia euclidiana. Esto forma k grupos inciales.\n",
    "\n",
    "4. **Recalcular los centroides:** una vez que todos los datos están asignados a los grupos, se calcula el centro promedio (centroide) de cada clúster. Los centroides se actualizan para reflejar las nuevas posiciones basadas en los puntos de datos asignados.\n",
    "\n",
    "5. **Repetir el proceso:** el proceso de asignación y recalculación de los centroides se repite hasta que las posiciones de los centroides no cambian significativamente entre iteraiones. En este punto, se dice que el algoritmo ha convergido.\n",
    "\n",
    "6. **Resultados:** el algoritmo *k-means* genera como salida los centroides de los k grupos y las etiquetas correspondientes cada punto de datos, indicando a qué clúster pertenece.\n",
    "\n",
    "\n",
    "El resultado final es una partición de los datos en k clústeres bien definidos, donde cada punto de datos está lo más cenrca posible de su centroide correspondiente."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
