{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ae75b3",
   "metadata": {},
   "source": [
    "# 11. Clasificación y Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06100379",
   "metadata": {},
   "source": [
    "## 11.1 Introducción a SVM\n",
    "\n",
    "### Algoritmo Support Vector Machine (SVM)\n",
    "\n",
    "El Spport Vector Machine (SVM) es un algoritmo de aprendizaje supervisado que puede utilizarse tanto para **clasificación** como para **regresión**. Su objetivo principal es encontrar el hiperplano que mejor separa las clases en un espacio de características, maximizando el **margen** entre las clases más cercanas, conocidas como los **vectores de soporte.**\n",
    "\n",
    "\n",
    "### Funcionamiento:\n",
    "\n",
    "El SVM proyecta los datos en un espacio de dimensiones más alto, buscando el hiperplano que maximice el margen entre las clases. Si los datos no son linealmente separables en el espacio original, el algoritmo puede utilizar funciones de kernel para transformar los datos, permitiendo una separación más eficaz en el nuevo espacio.\n",
    "\n",
    "\n",
    "### Tipos de problemas que resuelve:\n",
    "\n",
    "* **Clasificación:** el SVM se utiliza para poder resolver problemas de clasificación, donde asigna las instancias a una de las dos clases, encontrnado el hiperplano que las separa con el mayor margen posible. SVM se destaca por ser robusto frente a los datos con ruido y outliers, ya que optimiza el margen de separación.\n",
    "\n",
    "* **Regresión:** el SVM también se puede adaptar para resolver problemas de regresión mediante el uso de una variante conocida como Suppor Vector Regressión (SVR). En este caso, el algoritmo busca una línea o hiperplano que prediga un valor continuo, manteniendo la predicción lo más cerca posible de los valores reales, pero permitiendo una cierta cantidad de error (epsilon-insensitive loss).\n",
    "\n",
    "\n",
    "### Aplicaciones comunes:\n",
    "\n",
    "* **Clasificación de imágenes:** el SVM es ampliamente utilizado en la clasificación de imágenes, especialmente en casos donde las clases pueden separarse linealmente o mediante el uso de kernels.\n",
    "\n",
    "* **Análisis de texto:** en tareas como la clasificación de textos o detección de spam, el SVM es efectivo, especialmente cuando se utilizan técnicas como la vectorización de texto para representar los datos en un espacio de caracteristicas de alta dimensión.\n",
    "\n",
    "* **Reconocimiento de patrones:** el SVM es común en problemas de reconocimiento de patrones, tales como el reconocimiento de voz o la clasificación de genes en bioinformatica.\n",
    "\n",
    "\n",
    "### Ventajas del SVM:\n",
    "\n",
    "* **Eficiente en espacios de alta dimensión:** el SVM es eficaz cuando el número de dimensiones es mayor que el número de muestras.\n",
    "\n",
    "* **Robustez ante overfitting:** debido a su enfoque en maximizar el margen, el SVM es menos suceptible al overfitting, especialmente cuando se utiliza con un kernel adecuado.\n",
    "\n",
    "* **Adaptable:** mediante el uso de diferentes kernels (lineales, polinomiales, RBF), el SVM puede adaptarse a diferentes tipos de problemas no lineales.\n",
    "\n",
    "\n",
    "\n",
    "### Desventajas del SVM:\n",
    "\n",
    "* **Complejidad computacional:** el tiempo de entrenamiento puede ser considerable cuando el número de muestras es grande.\n",
    "\n",
    "* **Sensibilidad a la selección del kernel:** la elección del kernel adecuado es crucial para el rendimiento del modelo. Seleccionar un kernel inadecuado puede llevar a una clasificación incorrecta.\n",
    "\n",
    "\n",
    "### Conclusión:\n",
    "\n",
    "El SVM es una herramienta poderosa en el ámbito del *Machine Learning*, especialmente útil en problemas de clasificación y regresión con datos complejos. Su capacidad para trabajar en espacios de alta dimensión y su flexibilidad a través de diferentes kernels los hacen versátil en una amplia variedad de aplicaciones.\n",
    "\n",
    "\n",
    "## Definición de SVM\n",
    "\n",
    "### Máquinas de Soporte Vectorial (SVM)\n",
    "\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Las Máquinas de Soporte Vectorial (SVM) son algoritmos de aprendizaje supervisado utilizado tanto en problemas de clasificación como de regresión. El principio  fundamental del SVM se basa en la construcción de hiperplanos en un espacio de múltiples dimensiones para segmentar o separar los datos en diferentes clases.\n",
    "\n",
    "\n",
    "**Segmentación de Datos mediante Hiperplanos:**\n",
    "\n",
    "El objetivo principal de las SVM es encontrar el *hiperplano* que mejor separe las diferentes clases de datos en un espacio de características. Un hiperplano es un límite de decisión que segmenta el espacio en diferentes regiones, asignando una clase a cada lado del mismo. En el caso de problemas de clasificación, el SVM busca el hiperplano que maximice el *margen* entre las instancias más cercanas de cada clase. Estas instancias crecanas que definen el margen se conocen como *Vectores de Soporte*.\n",
    "\n",
    "El *margen* es la distancia entre el hiperplano y las instancias más cercanas de cualquier clase. Un margen más amplio entre las clases contribuye a una mejor generalización del modelo, reduciendo la probabilidad de error en futuras predicciones.\n",
    "\n",
    "### Hiperplanos en Espacios de Alta Dimensión:\n",
    "\n",
    "Cuando los datos no son linealmente separables en su espacio original, las SVM pueden utilizar una ténica conocida como *kernel trick*. Esta técnica transformalos datos a un espacio de dimensiones más alto donde sí pueden ser separados por un hiperplano lineal. Los kernels más utilizados incluyen:\n",
    "\n",
    "* **Kernel lineal:** apropiado para datos que pueden ser separados de manera lineal.\n",
    "\n",
    "* **Kernel polinomial:** introduce curvas en la separación de las clases.\n",
    "\n",
    "* **Kernel de base radial (RBF):** transforma los datos a un espacio infinito dimensional, siendo útil en casos donde no es posible separarlos linealmente en el espacio original.\n",
    "\n",
    "\n",
    "### Soft Margin & Hard Margin:\n",
    "\n",
    "En situaciones en las que los datos no son perfectamente separables, las SVM pueden utilizar una variación denominada *Soft Margin SVM*. Esta variante permite que algunos \n",
    "puntos se encuentren en el margen o incluso en el lado incorrecto del hiperplano, introducioendo una penalización por cada punto mal clasificado. EL *Hard Margin SVM* solo permite la separación perfecta sin errores, lo cual no es ideal en situaciones con datos ruidosos o con outliers, ya que puede llevar al overfitting.\n",
    "\n",
    "\n",
    "### Ejemplo visual:\n",
    "\n",
    "Imaginemos un escenario en el que tenemos dos clases de datos: puntos *rojos* y *verdes*. La SVM buscará una línea (en dos dimensiones) o un huperplano (en más dimensiones) que separe de mejor manera posible estos puntos, asegurándose de maximizar la distancia de los puntos más cercanos al límite de decisión. En este caso, los puntos que determinan la posición del hiperplano son los vectores de soporte.\n",
    "\n",
    "\n",
    "### Importancia del Margen:\n",
    "\n",
    "La clave de las SVM es que no solo encuentran una línea que sepaara las clases, sino que optimizan la posición de esta línea para maximizar el margen. Este enfoque de maximizar el margen contribuye a la robustez del modelo, ya que proporciona una mayor separación entre las clases y, por lo tanto, una mayor capacidad de generalización en nuevos datos.\n",
    "\n",
    "\n",
    "### Conclusión:\n",
    "\n",
    "Las Máquinas de Soporte Vectorial son una técnica de *Machine Learning* altamente efectiva para problemas de clasificación y regresión. Su fundamente en la construcción de hiperplano y la optimización del margen las hacen ideales para tareas donde los datos pueden ser separados de forma clara en diferentes clases. Además, la capacidad de menejar espacios de alta dimensión mediante el uso de kernels convierte al SVM en una herramienta versátil y poderosa para múltiples tipos de problemas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc3b75",
   "metadata": {},
   "source": [
    "## 11.2 Ejemplos y Aplicaciones\n",
    "\n",
    "### Fallos en las Clasificaciones y Separación Imperfecta de Puntos\n",
    "\n",
    "En los problemas de clasificación, uno de los principales desafíos es garantizar que los datos de diferentes clases se separen adecuadamente. Sin embargo, en la realidad, no siempre es posible trazar un ñímite perfecto que divida todas las clases de manera correcta. Esto puede dar lugar a clasificaciones erróneas, especialmente cuando los puntos no son linealmente separables o cuando existen outliers (puntos atípicos) que complican la creación de un modelo de clasificación preciso.\n",
    "\n",
    "### Clasificaciones Erróneas:\n",
    "\n",
    "Una clasificación errónea ocurre cuando un punto de datos se asigna a la clase equivocada debido a una falla en el modelo de separación. Estos errores pueden surgir por varias razones:\n",
    "\n",
    "* **Superposición de clases:** los puntos de diferentes clases pueden estar tan cercanos entre sí que es difícil encontrar un límite claro para separarlos.\n",
    "\n",
    "* **Outliers:** los puntos atípicos, que no siguen el patrón general de la mayoría de los datos, pueden desviar la línea de separación, afectando negativamente la clasificación.\n",
    "\n",
    "* **Ruido en los datos:** el ruido, o variabilidad aleatoria en los datos, también puede hacer que un modelo clasifique incorrectamente puntos que están cerca del límite de decisión.\n",
    "\n",
    "\n",
    "### Casos donde la Separación es Imperfecta:\n",
    "\n",
    "En la práctica, es común encontrar casos donde las clases no se pueden separar perfectamente mediante un hiperplano o una línea. Por ejemplo, en situaciones donde dos clases de datos se entrelazan o están muy cercanas, es probable que algunos puntos se encuentren mal clasificados. Estos cosos se manejan con enfoques como:\n",
    "\n",
    "* **Soft Margin SVM:** en lugar de intentar separar perfectamente os puntos, este método permite que algunos puntos queden dentro del margen o en el lado incorrecto de la separación, pero asignando una penalización por cada error. Esto evita que el modelo se sobreajuste (overfitting) a los datos de entrenamiento.\n",
    "\n",
    "* **Kernel Trick:** cuando las clases no son separables linealmente en su espacio original, el SVM utiliza funciones kernel para proyectar los datos a un espacio de dimensiones más alto, donde la separación puede ser más clara. A pesar de esto, pueden existir puntos que sigan quedando mal clasificados.\n",
    "\n",
    "\n",
    "### Ejemplos de Clasificaciones Erróneas:\n",
    "\n",
    "Supongamos un problema de clasificaión donde tenemos dos claes de puntos: *azules* y *rojos*. El objetivo es encontrar una línea que divida ambas clases. Sin embargo, si algunos puntos azules están muy cerca de los rojos, o incluso dentro de la región de los rojos, la clasificación puede fallar. Estos puntos cercanos al límite de decisión son los más propensos a ser clasificados incorrectamente.\n",
    "\n",
    "Imaginemos que trazamos una línea para separar estos puntos:\n",
    "\n",
    "* Los puntos a la izquierda de la línea se clasifican como azules, y los puntos a la derecha como rojos.\n",
    "* Si aparece un nuevo punto en la parte inferior izquierda, podría clasificarse incorrectamente como azul, aunque en realidad debería ser rojo.\n",
    "\n",
    "Este tipo de errores demuestra que en muchos casos no es posible establecer una frontera de clasificación perfecta, y es ahí donde entran en juego estrategias que permitan cierta flexibilidad, como el soft margin y la optimización de hiperparámetros.\n",
    "\n",
    "\n",
    "### Manejo de Clasificaciones Imperfectas:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
