{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9ae75b3",
   "metadata": {},
   "source": [
    "# 11. Clasificación y Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06100379",
   "metadata": {},
   "source": [
    "## 11.1 Introducción a SVM\n",
    "\n",
    "### Algoritmo Support Vector Machine (SVM)\n",
    "\n",
    "El Spport Vector Machine (SVM) es un algoritmo de aprendizaje supervisado que puede utilizarse tanto para **clasificación** como para **regresión**. Su objetivo principal es encontrar el hiperplano que mejor separa las clases en un espacio de características, maximizando el **margen** entre las clases más cercanas, conocidas como los **vectores de soporte.**\n",
    "\n",
    "\n",
    "### Funcionamiento:\n",
    "\n",
    "El SVM proyecta los datos en un espacio de dimensiones más alto, buscando el hiperplano que maximice el margen entre las clases. Si los datos no son linealmente separables en el espacio original, el algoritmo puede utilizar funciones de kernel para transformar los datos, permitiendo una separación más eficaz en el nuevo espacio.\n",
    "\n",
    "\n",
    "### Tipos de problemas que resuelve:\n",
    "\n",
    "* **Clasificación:** el SVM se utiliza para poder resolver problemas de clasificación, donde asigna las instancias a una de las dos clases, encontrnado el hiperplano que las separa con el mayor margen posible. SVM se destaca por ser robusto frente a los datos con ruido y outliers, ya que optimiza el margen de separación.\n",
    "\n",
    "* **Regresión:** el SVM también se puede adaptar para resolver problemas de regresión mediante el uso de una variante conocida como Suppor Vector Regressión (SVR). En este caso, el algoritmo busca una línea o hiperplano que prediga un valor continuo, manteniendo la predicción lo más cerca posible de los valores reales, pero permitiendo una cierta cantidad de error (epsilon-insensitive loss).\n",
    "\n",
    "\n",
    "### Aplicaciones comunes:\n",
    "\n",
    "* **Clasificación de imágenes:** el SVM es ampliamente utilizado en la clasificación de imágenes, especialmente en casos donde las clases pueden separarse linealmente o mediante el uso de kernels.\n",
    "\n",
    "* **Análisis de texto:** en tareas como la clasificación de textos o detección de spam, el SVM es efectivo, especialmente cuando se utilizan técnicas como la vectorización de texto para representar los datos en un espacio de caracteristicas de alta dimensión.\n",
    "\n",
    "* **Reconocimiento de patrones:** el SVM es común en problemas de reconocimiento de patrones, tales como el reconocimiento de voz o la clasificación de genes en bioinformatica.\n",
    "\n",
    "\n",
    "### Ventajas del SVM:\n",
    "\n",
    "* **Eficiente en espacios de alta dimensión:** el SVM es eficaz cuando el número de dimensiones es mayor que el número de muestras.\n",
    "\n",
    "* **Robustez ante overfitting:** debido a su enfoque en maximizar el margen, el SVM es menos suceptible al overfitting, especialmente cuando se utiliza con un kernel adecuado.\n",
    "\n",
    "* **Adaptable:** mediante el uso de diferentes kernels (lineales, polinomiales, RBF), el SVM puede adaptarse a diferentes tipos de problemas no lineales.\n",
    "\n",
    "\n",
    "\n",
    "### Desventajas del SVM:\n",
    "\n",
    "* **Complejidad computacional:** el tiempo de entrenamiento puede ser considerable cuando el número de muestras es grande.\n",
    "\n",
    "* **Sensibilidad a la selección del kernel:** la elección del kernel adecuado es crucial para el rendimiento del modelo. Seleccionar un kernel inadecuado puede llevar a una clasificación incorrecta.\n",
    "\n",
    "\n",
    "### Conclusión:\n",
    "\n",
    "El SVM es una herramienta poderosa en el ámbito del *Machine Learning*, especialmente útil en problemas de clasificación y regresión con datos complejos. Su capacidad para trabajar en espacios de alta dimensión y su flexibilidad a través de diferentes kernels los hacen versátil en una amplia variedad de aplicaciones.\n",
    "\n",
    "\n",
    "## Definición de SVM\n",
    "\n",
    "### Máquinas de Soporte Vectorial (SVM)\n",
    "\n",
    "\n",
    "**Definición:**\n",
    "\n",
    "Las Máquinas de Soporte Vectorial (SVM) son algoritmos de aprendizaje supervisado utilizado tanto en problemas de clasificación como de regresión. El principio  fundamental del SVM se basa en la construcción de hiperplanos en un espacio de múltiples dimensiones para segmentar o separar los datos en diferentes clases.\n",
    "\n",
    "\n",
    "**Segmentación de Datos mediante Hiperplanos:**\n",
    "\n",
    "El objetivo principal de las SVM es encontrar el *hiperplano* que mejor separe las diferentes clases de datos en un espacio de características. Un hiperplano es un límite de decisión que segmenta el espacio en diferentes regiones, asignando una clase a cada lado del mismo. En el caso de problemas de clasificación, el SVM busca el hiperplano que maximice el *margen* entre las instancias más cercanas de cada clase. Estas instancias crecanas que definen el margen se conocen como *Vectores de Soporte*.\n",
    "\n",
    "El *margen* es la distancia entre el hiperplano y las instancias más cercanas de cualquier clase. Un margen más amplio entre las clases contribuye a una mejor generalización del modelo, reduciendo la probabilidad de error en futuras predicciones.\n",
    "\n",
    "### Hiperplanos en Espacios de Alta Dimensión:\n",
    "\n",
    "Cuando los datos no son linealmente separables en su espacio original, las SVM pueden utilizar una ténica conocida como *kernel trick*. Esta técnica transformalos datos a un espacio de dimensiones más alto donde sí pueden ser separados por un hiperplano lineal. Los kernels más utilizados incluyen:\n",
    "\n",
    "* **Kernel lineal:** apropiado para datos que pueden ser separados de manera lineal.\n",
    "\n",
    "* **Kernel polinomial:** introduce curvas en la separación de las clases.\n",
    "\n",
    "* **Kernel de base radial (RBF):** transforma los datos a un espacio infinito dimensional, siendo útil en casos donde no es posible separarlos linealmente en el espacio original.\n",
    "\n",
    "\n",
    "### Soft Margin & Hard Margin:\n",
    "\n",
    "En situaciones en las que los datos no son perfectamente separables, las SVM pueden utilizar una variación denominada *Soft Margin SVM*. Esta variante permite que algunos \n",
    "puntos se encuentren en el margen o incluso en el lado incorrecto del hiperplano, introducioendo una penalización por cada punto mal clasificado. EL *Hard Margin SVM* solo permite la separación perfecta sin errores, lo cual no es ideal en situaciones con datos ruidosos o con outliers, ya que puede llevar al overfitting.\n",
    "\n",
    "\n",
    "### Ejemplo visual:\n",
    "\n",
    "Imaginemos un escenario en el que tenemos dos clases de datos: puntos *rojos* y *verdes*. La SVM buscará una línea (en dos dimensiones) o un huperplano (en más dimensiones) que separe de mejor manera posible estos puntos, asegurándose de maximizar la distancia de los puntos más cercanos al límite de decisión. En este caso, los puntos que determinan la posición del hiperplano son los vectores de soporte.\n",
    "\n",
    "\n",
    "### Importancia del Margen:\n",
    "\n",
    "La clave de las SVM es que no solo encuentran una línea que sepaara las clases, sino que optimizan la posición de esta línea para maximizar el margen. Este enfoque de maximizar el margen contribuye a la robustez del modelo, ya que proporciona una mayor separación entre las clases y, por lo tanto, una mayor capacidad de generalización en nuevos datos.\n",
    "\n",
    "\n",
    "### Conclusión:\n",
    "\n",
    "Las Máquinas de Soporte Vectorial son una técnica de *Machine Learning* altamente efectiva para problemas de clasificación y regresión. Su fundamente en la construcción de hiperplano y la optimización del margen las hacen ideales para tareas donde los datos pueden ser separados de forma clara en diferentes clases. Además, la capacidad de menejar espacios de alta dimensión mediante el uso de kernels convierte al SVM en una herramienta versátil y poderosa para múltiples tipos de problemas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc3b75",
   "metadata": {},
   "source": [
    "## 11.2 Ejemplos y Aplicaciones\n",
    "\n",
    "### Fallos en las Clasificaciones y Separación Imperfecta de Puntos\n",
    "\n",
    "En los problemas de clasificación, uno de los principales desafíos es garantizar que los datos de diferentes clases se separen adecuadamente. Sin embargo, en la realidad, no siempre es posible trazar un ñímite perfecto que divida todas las clases de manera correcta. Esto puede dar lugar a clasificaciones erróneas, especialmente cuando los puntos no son linealmente separables o cuando existen outliers (puntos atípicos) que complican la creación de un modelo de clasificación preciso.\n",
    "\n",
    "### Clasificaciones Erróneas:\n",
    "\n",
    "Una clasificación errónea ocurre cuando un punto de datos se asigna a la clase equivocada debido a una falla en el modelo de separación. Estos errores pueden surgir por varias razones:\n",
    "\n",
    "* **Superposición de clases:** los puntos de diferentes clases pueden estar tan cercanos entre sí que es difícil encontrar un límite claro para separarlos.\n",
    "\n",
    "* **Outliers:** los puntos atípicos, que no siguen el patrón general de la mayoría de los datos, pueden desviar la línea de separación, afectando negativamente la clasificación.\n",
    "\n",
    "* **Ruido en los datos:** el ruido, o variabilidad aleatoria en los datos, también puede hacer que un modelo clasifique incorrectamente puntos que están cerca del límite de decisión.\n",
    "\n",
    "\n",
    "### Casos donde la Separación es Imperfecta:\n",
    "\n",
    "En la práctica, es común encontrar casos donde las clases no se pueden separar perfectamente mediante un hiperplano o una línea. Por ejemplo, en situaciones donde dos clases de datos se entrelazan o están muy cercanas, es probable que algunos puntos se encuentren mal clasificados. Estos cosos se manejan con enfoques como:\n",
    "\n",
    "* **Soft Margin SVM:** en lugar de intentar separar perfectamente os puntos, este método permite que algunos puntos queden dentro del margen o en el lado incorrecto de la separación, pero asignando una penalización por cada error. Esto evita que el modelo se sobreajuste (overfitting) a los datos de entrenamiento.\n",
    "\n",
    "* **Kernel Trick:** cuando las clases no son separables linealmente en su espacio original, el SVM utiliza funciones kernel para proyectar los datos a un espacio de dimensiones más alto, donde la separación puede ser más clara. A pesar de esto, pueden existir puntos que sigan quedando mal clasificados.\n",
    "\n",
    "\n",
    "### Ejemplos de Clasificaciones Erróneas:\n",
    "\n",
    "Supongamos un problema de clasificaión donde tenemos dos claes de puntos: *azules* y *rojos*. El objetivo es encontrar una línea que divida ambas clases. Sin embargo, si algunos puntos azules están muy cerca de los rojos, o incluso dentro de la región de los rojos, la clasificación puede fallar. Estos puntos cercanos al límite de decisión son los más propensos a ser clasificados incorrectamente.\n",
    "\n",
    "Imaginemos que trazamos una línea para separar estos puntos:\n",
    "\n",
    "* Los puntos a la izquierda de la línea se clasifican como azules, y los puntos a la derecha como rojos.\n",
    "* Si aparece un nuevo punto en la parte inferior izquierda, podría clasificarse incorrectamente como azul, aunque en realidad debería ser rojo.\n",
    "\n",
    "Este tipo de errores demuestra que en muchos casos no es posible establecer una frontera de clasificación perfecta, y es ahí donde entran en juego estrategias que permitan cierta flexibilidad, como el soft margin y la optimización de hiperparámetros.\n",
    "\n",
    "\n",
    "### Manejo de Clasificaciones Imperfectas:\n",
    "\n",
    "El SVM introduce el concepto de *márgenes suaves (soft margins)* para manejar este tipo de situaciones. Esto signfica que el modelo puede tolerar algunos errores de clasificación, especialmente si mejorar la precisión en un conjunto de datos resulta en una menor generalización a datos nuevos.\n",
    "\n",
    "Además, los *outliers* son una causa común de errores en la clasificación. Estos puntos extremos pueden desviar el hiperplano de separación, lo que puede llevar a errores en puntos que, sin la influencia de los outliers, habrían sido clasificados correctamente. En estos casos, las SVM permiten ajustar el nivel de tolerancia a errores mediante un parámetro que controla la penalización por puntos mal clasificados.\n",
    "\n",
    "\n",
    "### Conclusión:\n",
    "\n",
    "Las clasificaciones erróneas son inevitables cuando los datos no se pueden separar perfectamente. El SVM maneja este problema mediante la optimización de los márgenes y la introducción de técnicas como el *kernel trick* para mejorar la separación en espacios de mayor dimensión. Sin embargo, en casos con putliers o ruido, es esencial encontrar un equilibrio entre la precisión del modelo y su capacidad para generalizar sin sobreajustarse.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7059a68",
   "metadata": {},
   "source": [
    "## 11.3 Casos Prácticos y Mejoras\n",
    "\n",
    "### Estrategias y Técnicas para Mejorar la Precisión de SVM\n",
    "\n",
    "**Introducción:** \n",
    "\n",
    "El rendimiento de un algoritmo de Máquinas de Soporte Vectorial (SVM) puede mojorarse significativamente mediante la aplicación de diversas estrategias y técnicas. Entre ellas se destacan el uso de *kernels* para ttransformar los datos y el *ajuste de hiperparámetros* para optimizar el modelo. Estas técnicas permiten que el SVM sea más flexible y preciso, especialmente en situaciones donde los datos no son linealmente separables.\n",
    "\n",
    "\n",
    "**Uso de Kernels:**\n",
    "\n",
    "Uno de los aspectos más poderosos del SVM es su capacidad de utilizar *kernels*, funciones que transforman los datos a un espacio de caracteristicas de mayor dimensión. Esta transformación permite que problemas que no son separables linealmente en su espacio original puedan ser separados mediante un hiperplano en un espacio proyectado. Existen varios tipos de kernels, cada uno con sus características y aplicaciones específicas:\n",
    "\n",
    "* **Kernel Lineal:** es el más sencillo y se utiliza cuando los datos son aproximadamente linealmente separables. Es eficiente en casos donde el número de caracteristicas es grande en comparación con el número de observaciones.\n",
    "\n",
    "* **Kernel Polinomial:** este kernel es útil cuando los datos tienen una estructura no lineal, pero que puede representarse mediante una curva polinómica. Introduce no linealidad en la separación, permitiendo la clasificación de datos complejos.\n",
    "\n",
    "* **Kernel de Base Radial (RBF):** el kernel RBF, también conocido como kernel Gaussiano, es ampliamente utilizado cuando no se tiene una idea clara de la forma en que los datos pueden separarse. Este kernel proyecta los datos a un espacio infinito dimensional, lo que lo hace extremadamente poderoso en problemas donde los límites entre clases son muy complejos.\n",
    "\n",
    "\n",
    "El  uso adecuado del kernel puede transformar un problema aparentemente intratable en uno que el SVM puede resolver con alta precisión, ya que permite encontrar un *hiperplano* en un espacio transformado donde las clases son más fácilmente separables.\n",
    "\n",
    "\n",
    "**Ajuste de Hiperparámetros:**\n",
    "\n",
    "El ajuste de los *hiperparámetros* es crucial para mejorar la precisión de los modelos de SVM. Los principales hiperparámetros que deben optimizarse incluyen:\n",
    "\n",
    "* **Parámetro C:** el parámetro C controla el *trade-off* entre maximizar el margen y minimizar los errores de clasificación. Un valor de C pequeño permite un margen más amplio, lo que implica mayor tolerancia a errores de clasificación en el conjunto de entrenamiento, favoreciendo una mayor generealización. Por el contrario, un valor de C alto intenta clasificar correctamente la mayor cantidad de puntos en el conjunto de entrenamiento, a expensas de un margen más estrecho, lo que puede llevar a sobreajustar el modelo.\n",
    "    * **C bajo:** margen amplio, tolerancia a errores en los datos de entrenamiento, mejor generalización.\n",
    "    * **C alto:** margen estrecho, clasificación precisa en los datos de entrenamiento, mayor riesgo de sobreajuste.\n",
    "\n",
    "\n",
    "* **Gamma en el Kernel RBF:** este parámetro determina el alcance de la influencia de un solo punto de datos. Un valor de gamma bajo significa que un solo punto de datos tiene un efecto amplio, lo que puede llevar a una separación más suave de las clases. Un valor de gamma alto limita el alcance de los puntos individuales, lo que puede crear límites más complejos y ajustar demasiado el modelo a los datos de entrenamiento.\n",
    "    * **Gamma bajo:** efecto global, límites más suaves, menor riesgo de sobreajuste.\n",
    "    * **Gamma alto:** efecto local, límites más complejos, mayor riesgo de sobreajuste.\n",
    "\n",
    "\n",
    "**Validación Cruzada:**\n",
    "\n",
    "Para asegurarse de que los hiperparámetros seleccionados (como C y gamma) optimizan el rendimiento del modelo, es común utilizar técnicas de validación cruzada. Esto implica dividir el conjunto de datos en varias particiones, entrenar el modelo en una parte y probarlo en otra. Este proceso se repite varias veces para garantizar que los resultados sean consistenten y no dependan de un solo conjunto de datos de prueba. La validación cruzada permite ajustar los hiperpar+ametros de manera más precisa, garantizando una mejor generalización del modelo a nuevos datos.\n",
    "\n",
    "\n",
    "**Optimización mediante Grid Search:**\n",
    "\n",
    "Una técnica común para ajustar los hiperparámetros es el *Grid Search*, que explora sistemáticamente diferentes combinaciones de valores para los hiperparámetros clave (como C y gamma). Este proceso encuentra los valores que maximizan la precisión del modelos mediante pruebas exhaustivas en diferentes configuraciones. Aunque puede ser computacionalmente costoso, Grid Research garantiza que se exploren todas las opciones posibles para encontrar la mejor combinación.\n",
    "\n",
    "\n",
    "**Manejo de Outliers:**\n",
    "\n",
    "Los *outlieers* son puntos de datos que se encuentran fuera de los patrones esperados y pueden afectar negativamente la precisión de un modelo de SVM. Para manejar los outliers, se puede ajustar el parámetro C para permitir que algunos puntos queden mal calsificados, lo que evita que el modelo se ajuste exesivamente a puntos ruidosos o atípicos. Esto se conoce como *soft margin*, que introduce una tolerancia controlada a los errores de clasificaión. \n",
    "\n",
    "\n",
    "* **Conclusión:**\n",
    "\n",
    "Mejorar la precisión de un modelo de SVM requiere una combinación de estratégias bien implemntedas, como la elección adecuada de kernels, el ajsute de hiperparámetros, y el uso de validación cruzada. El kernel correcto permite transformar os datos para que sean más fácilmente separables, mietras que el ajuste de parámetros como C y gamma optimizan el modelo para mejorar su rendimiento sin caer en el sobreajuste. Estas ténicas, combinadas con el manejo de outliers y la optimización de hiperparámetros, aseguran que el SVM funcione de manera eficaz incluso en problemas complejos.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
